{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EasyMatchy.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV8yQblJhQwY",
        "colab_type": "text"
      },
      "source": [
        "**Exploratory data analysis (EDA)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHrQSb9_nTR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "# load the dataset\n",
        "dataset = pandas.read_csv('ClinicalNotes', delimiter = '\\t')\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1AVE1jbnhJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fetch wordcount for each abstract\n",
        "dataset['word_count'] = dataset['notes'].apply(lambda x: len(str(x).split(\" \")))\n",
        "dataset[['notes','word_count']].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJtaE-U-nkE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Descriptive statistics of word counts\n",
        "dataset.word_count.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhxq7QD0npje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Identify common words\n",
        "freq = pandas.Series(' '.join(dataset['notes']).split()).value_counts()[:20]\n",
        "freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2pzYgxnntoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Identify uncommon words\n",
        "freq1 =  pandas.Series(' '.join(dataset \n",
        "         ['notes']).split()).value_counts()[-20:]\n",
        "freq1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hopf-aiQnygu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "stem = PorterStemmer()\n",
        "word = \"inversely\"\n",
        "print(\"stemming:\",stem.stem(word))\n",
        "print(\"lemmatization:\", lem.lemmatize(word, \"v\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k22rlneHn3qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries for text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#nltk.download('wordnet') \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFQSSlAxn6cP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Creating a list of stop words and adding custom stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"mL\", \"and \", \"of\", \"to\", \"AM\", \"with\", \"/\", \"ml'\", \"mg\", \"dl\", \"dl'\", \"meq\", \"pm\", \"the\", \"in\", \"mEq/L\"]\n",
        "stop_words = stop_words.union(new_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1ony9Wyn9om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for i in range(0, 3644):\n",
        "    #Remove punctuations\n",
        "    text = re.sub('[^a-zA-Z]', ' ', dataset['articles'][i])\n",
        "    \n",
        "    #Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    ##Stemming\n",
        "    ps=PorterStemmer()\n",
        "    #Lemmatisation\n",
        "    lem = WordNetLemmatizer()\n",
        "    text = [lem.lemmatize(word) for word in text if not word in  \n",
        "            stop_words] \n",
        "    text = \" \".join(text)\n",
        "    corpus.append(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnTF6opuoBFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word cloud\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stop_words,\n",
        "                          max_words=100,\n",
        "                          max_font_size=50, \n",
        "                          random_state=42\n",
        "                         ).generate(str(corpus))\n",
        "print(wordcloud)\n",
        "plt.figure(figsize=(15,10))\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "fig.savefig(\"word1.png\", dpi=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUNkH414oD7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
        "X=cv.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9O4j4rYoG7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(cv.vocabulary_.keys())[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMAX6SWfoKZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Most frequently occuring words\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
        "                   vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                       reverse=True)\n",
        "    return words_freq[:n]\n",
        "#Convert most freq words to dataframe for plotting bar plot\n",
        "top_words = get_top_n_words(corpus, n=20)\n",
        "top_df = pandas.DataFrame(top_words)\n",
        "top_df.columns=[\"Word\", \"Freq\"]\n",
        "#Barplot of most freq words\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
        "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xu7s9lBoOOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Most frequently occuring Bi-grams\n",
        "def get_top_n2_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
        "            max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
        "                  vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                reverse=True)\n",
        "    return words_freq[:n]\n",
        "top2_words = get_top_n2_words(corpus, n=20)\n",
        "top2_df = pandas.DataFrame(top2_words)\n",
        "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
        "print(top2_df)\n",
        "#Barplot of most freq Bi-grams\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(13,18)})\n",
        "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\n",
        "h.set_xticklabels(h.get_xticklabels(), rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaFr6xA-oYJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Most frequently occuring Tri-grams\n",
        "def get_top_n3_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
        "           max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
        "                  vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                reverse=True)\n",
        "    return words_freq[:n]\n",
        "top3_words = get_top_n3_words(corpus, n=20)\n",
        "top3_df = pandas.DataFrame(top3_words)\n",
        "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
        "print(top3_df)\n",
        "#Barplot of most freq Tri-grams\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(15,8)})\n",
        "j=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
        "j.set_xticklabels(j.get_xticklabels(), rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4pd00M-onC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        " \n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(X)\n",
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        " \n",
        "# fetch document for which keywords needs to be extracted\n",
        "doc=corpus[200]\n",
        " \n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}